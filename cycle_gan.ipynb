{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"ws1EcwoTcBVI"},"outputs":[],"source":["# download dataset and unzip it to train\n","!wget https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/horse2zebra.zip\n","!unzip horse2zebra.zip"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3016,"status":"ok","timestamp":1661578996024,"user":{"displayName":"Hasan Ali Emon","userId":"08410466069573643183"},"user_tz":-360},"id":"FxNlbJ2MP0zJ","outputId":"671398f8-7a0a-4aa8-fbeb-893207199f27"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([5, 1, 30, 30])\n"]}],"source":["\n","#discriminator model\n","import torch\n","import torch.nn as nn\n","\n","class Block(nn.Module):\n","    def __init__(self, in_channels, out_channels, stride):\n","        super().__init__()\n","\n","        self.conv = nn.Sequential(\n","            nn.Conv2d(in_channels,\n","                out_channels, \n","                kernel_size = 4, \n","                stride = stride, \n","                padding = 1, \n","                bias=True, \n","                padding_mode='reflect'), #padding model reflect helps to deal with artifacts\n","            nn.InstanceNorm2d(out_channels),\n","            nn.LeakyReLU(0.2)\n","        )\n","    def forward(self, x):\n","        return self.conv(x)\n","\n","class Discriminator(nn.Module):\n","    def __init__(self, in_channels = 3, features = [64, 128, 256, 512]):\n","        super().__init__()\n","\n","        self.initial = nn.Sequential(\n","            nn.Conv2d(\n","                in_channels,\n","                features[0],\n","                kernel_size = 4,\n","                stride = 2,\n","                padding = 1,\n","                padding_mode = 'reflect'\n","            ),\n","            nn.LeakyReLU(0.2)\n","        )\n","\n","        layers = []\n","        in_channels = features[0]\n","        for feature in features[1:]:\n","            layers.append(Block(in_channels, feature, stride = 1 if feature == features[-1] else 2))\n","            in_channels = feature\n","        layers.append(nn.Conv2d(\n","            in_channels, \n","            out_channels = 1, \n","            kernel_size=4,\n","            stride = 1,\n","            padding = 1,\n","            padding_mode = 'reflect'\n","        ))\n","\n","        self.model = nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        x = self.initial(x)\n","        return torch.sigmoid(self.model(x))\n","    \n","\n","def test():\n","    x = torch.randn((5, 3, 256, 256))\n","    model = Discriminator(in_channels = 3)\n","    pred = model(x)\n","    print(pred.shape)\n","test()"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":565,"status":"ok","timestamp":1661578996582,"user":{"displayName":"Hasan Ali Emon","userId":"08410466069573643183"},"user_tz":-360},"id":"1oqazDsbP_nr","outputId":"c3e00ee0-a079-491c-e90c-c14fba352e07"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 3, 256, 256])\n"]}],"source":["# generator\n","\n","class ConvBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, down=True, use_act = True, **kwargs):\n","        super().__init__()\n","\n","        self.conv = nn.Sequential(\n","            nn.Conv2d(in_channels, out_channels, padding_mode= 'reflect', **kwargs)\n","            if down\n","            else nn.ConvTranspose2d(in_channels, out_channels, **kwargs),\n","            nn.InstanceNorm2d(out_channels),\n","            nn.ReLU(inplace=True) if use_act else nn.Identity()\n","        )\n","\n","    def forward(self, x):\n","        return self.conv(x)\n","\n","class ResidualBlock(nn.Module):\n","    def __init__(self, channels):\n","        super().__init__()\n","        self.block = nn.Sequential(\n","            ConvBlock(channels, channels, kernel_size=3, padding=1),\n","            ConvBlock(channels, channels, use_act = False, kernel_size = 3, padding=1)\n","        )\n","    \n","    def forward(self, x):\n","        return x + self.block(x)\n","\n","class Generator(nn.Module):\n","    def __init__(self, img_channels, num_features = 64, num_residuals = 9):\n","        super().__init__()\n","        self.initial = nn.Sequential(\n","            nn.Conv2d(img_channels, num_features, kernel_size = 7, stride = 1, padding = 3, padding_mode = 'reflect'),\n","            nn.ReLU(inplace=True)\n","        )\n","        self.down_blocks = nn.ModuleList([\n","            ConvBlock(num_features, num_features*2, kernel_size=3, stride = 2, padding = 1),\n","            ConvBlock(num_features*2, num_features*4, kernel_size=3, stride = 2, padding = 1)\n","        ])\n","        self.residual_blocks = nn.Sequential(\n","            *[ResidualBlock(num_features*4) for _ in range(num_residuals)]\n","        )\n","        self.up_blocks = nn.ModuleList([\n","            ConvBlock(num_features*4, num_features*2, down=False, kernel_size = 3, stride = 2, padding = 1, output_padding=1),\n","            ConvBlock(num_features*2, num_features, down=False, kernel_size = 3, stride = 2, padding = 1, output_padding=1),\n","        ])\n","        self.last = nn.Conv2d(num_features*1, img_channels, kernel_size = 7, stride = 1, padding=3, padding_mode='reflect')\n","\n","    def forward(self, x):\n","        x = self.initial(x)\n","        for layer in self.down_blocks:\n","            x= layer(x)\n","        x = self.residual_blocks(x)\n","        for layer in self.up_blocks:\n","            x = layer(x)\n","        return torch.tanh(self.last(x))\n","\n","\n","def test():\n","    img_channels = 3\n","    img_size = 256\n","    x = torch.randn((2, img_channels, img_size, img_size))\n","    gen = Generator(img_channels, 9)\n","    print(gen(x).shape)\n","test()"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1661579703365,"user":{"displayName":"Hasan Ali Emon","userId":"08410466069573643183"},"user_tz":-360},"id":"9H7IcVV3bgF0"},"outputs":[],"source":["# config file\n","# import torch\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","\n","DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n","TRAIN_DIR = '/content/horse2zebra'\n","VAL_DIR = '/content/horse2zebra'\n","BATCH_SIZE = 1\n","LEARNING_RATE = 1e-5\n","LAMBDA_IDENTITY = 0.0\n","LAMBDA_CYCLE = 10\n","NUM_WORKERS = 4\n","NUM_EPOCHES = 10\n","LOAD_MODEL = False\n","SAVE_MODEL = True\n","CHECKPOINT_GEN_H = '/content/cpt/gen_horse.pth.tar'\n","CHECKPOINT_GEN_Z = '/content/cpt/gen_zebra.pth.tar'\n","CHECKPOINT_CRITIC_H = '/content/cpt/critic_horse.pth.tar'\n","CHECKPOINT_CRITIC_Z = '/content/cpt/critic_zebra.pth.tar'\n","\n","transforms = A.Compose(\n","    [\n","        A.Resize(width=256, height=256),\n","        A.HorizontalFlip(p=0.5),\n","        A.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5], max_pixel_value=255),\n","        ToTensorV2(),\n","    ],\n","    additional_targets = {'image0': 'image'}\n",")"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1661579705459,"user":{"displayName":"Hasan Ali Emon","userId":"08410466069573643183"},"user_tz":-360},"id":"DQKV7qfzdQYV"},"outputs":[],"source":["#@title Default title text\n","# utils.py\n","\n","import random, torch, os\n","import numpy as np\n","# import config\n","import copy\n","\n","def save_checkpoint(model, optimizer, filename = 'my_checkpoint.pth.tar'):\n","    print('{} \\n saving checkpoint'.format('=*'*25))\n","    checkpoint = {\n","        'state_dict': model.state_dict(),\n","        'optimizer_dict': optimizer.state_dict()\n","    }\n","\n","    torch.save(checkpoint, filename)\n","\n","def load_checkpoint(checkpoint_file, model, optimizer, lr):\n","    print('{} \\n loading checkpoint'.format('=*'*25))\n","    checkpoint = torch.load(checkpoint_file, map_location=DEVICE)\n","    model.load_state_dict(checkpoint['state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer_dict'])\n","    # including learning rate for stable training\n","    for param in optimizer.param_groups:\n","        param['lr'] = lr\n","\n","def seed_everything(seed=42):\n","    os.environment['PYTHONHASHSEED'] = str(seed)\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministics = True\n","    torch.backends.cudnn.benchmarcks = False"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1661579705981,"user":{"displayName":"Hasan Ali Emon","userId":"08410466069573643183"},"user_tz":-360},"id":"xY90StfnTcrh"},"outputs":[],"source":["# dataset loading\n","\n","import torch\n","from torch.utils.data import Dataset\n","from PIL import Image\n","import os\n","\n","# import config\n","\n","class HorseZebraDataset(Dataset):\n","    def __init__(self, root_horse, root_zebra, transform=None):\n","        self.root_horse = root_horse\n","        self.root_zebra = root_zebra\n","        self.transform = transform\n","\n","        self.horse_images = os.listdir(self.root_horse)\n","        self.zebra_images = os.listdir(self.root_zebra)\n","        self.horse_len = len(self.horse_images)\n","        self.zebra_len = len(self.zebra_images)\n","        self.length_dataset = max(len(self.horse_images), len(self.zebra_images))\n","\n","    def __len__(self):\n","        return self.length_dataset\n","\n","    def __getitem__(self, index):\n","        horse_img = self.horse_images[index % self.horse_len]\n","        zebra_img = self.zebra_images[index % self.zebra_len]\n","\n","        horse_path = os.path.join(self.root_horse, horse_img)\n","        zebra_path = os.path.join(self.root_zebra, zebra_img)\n","\n","        horse_img = np.array(Image.open(horse_path).convert('RGB'))\n","        zebra_img = np.array(Image.open(zebra_path).convert('RGB'))\n","\n","        if self.transform:\n","            augmentations = self.transform(image=zebra_img, image0=horse_img)\n","            zebra_img = augmentations['image']\n","            horse_img = augmentations['image0']\n","        \n","        return zebra_img, horse_img"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sWVZVwUa1Q_F","executionInfo":{"status":"ok","timestamp":1661582944942,"user_tz":-360,"elapsed":3237222,"user":{"displayName":"Hasan Ali Emon","userId":"08410466069573643183"}},"outputId":"91af3e10-3869-42f0-d67a-ef7a7dcc80e8"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 1334/1334 [05:19<00:00,  4.17it/s, H_fake=0, H_real=0]\n"]},{"output_type":"stream","name":"stdout","text":["=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=* \n"," saving checkpoint\n","=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=* \n"," saving checkpoint\n","=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=* \n"," saving checkpoint\n","=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=* \n"," saving checkpoint\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1334/1334 [05:22<00:00,  4.14it/s, H_fake=0, H_real=0]\n"]},{"output_type":"stream","name":"stdout","text":["=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=* \n"," saving checkpoint\n","=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=* \n"," saving checkpoint\n","=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=* \n"," saving checkpoint\n","=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=* \n"," saving checkpoint\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1334/1334 [05:22<00:00,  4.14it/s, H_fake=0, H_real=0]\n"]},{"output_type":"stream","name":"stdout","text":["=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=* \n"," saving checkpoint\n","=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=* \n"," saving checkpoint\n","=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=* \n"," saving checkpoint\n","=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=* \n"," saving checkpoint\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1334/1334 [05:22<00:00,  4.14it/s, H_fake=0, H_real=0]\n"]},{"output_type":"stream","name":"stdout","text":["=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=* \n"," saving checkpoint\n","=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=* \n"," saving checkpoint\n","=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=* \n"," saving checkpoint\n","=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=* \n"," saving checkpoint\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1334/1334 [05:21<00:00,  4.14it/s, H_fake=0, H_real=0]\n"]},{"output_type":"stream","name":"stdout","text":["=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=* \n"," saving checkpoint\n","=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=* \n"," saving checkpoint\n","=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=* \n"," saving checkpoint\n","=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=* \n"," saving checkpoint\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1334/1334 [05:21<00:00,  4.14it/s, H_fake=0, H_real=0]\n"]},{"output_type":"stream","name":"stdout","text":["=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=* \n"," saving checkpoint\n","=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=* \n"," saving checkpoint\n","=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=* \n"," saving checkpoint\n","=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=* \n"," saving checkpoint\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1334/1334 [05:21<00:00,  4.15it/s, H_fake=0, H_real=0]\n"]},{"output_type":"stream","name":"stdout","text":["=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=* \n"," saving checkpoint\n","=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=* \n"," saving checkpoint\n","=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=* \n"," saving checkpoint\n","=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=* \n"," saving checkpoint\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1334/1334 [05:22<00:00,  4.14it/s, H_fake=0, H_real=0]\n"]},{"output_type":"stream","name":"stdout","text":["=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=* \n"," saving checkpoint\n","=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=* \n"," saving checkpoint\n","=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=* \n"," saving checkpoint\n","=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=* \n"," saving checkpoint\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1334/1334 [05:22<00:00,  4.14it/s, H_fake=0, H_real=0]\n"]},{"output_type":"stream","name":"stdout","text":["=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=* \n"," saving checkpoint\n","=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=* \n"," saving checkpoint\n","=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=* \n"," saving checkpoint\n","=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=* \n"," saving checkpoint\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1334/1334 [05:21<00:00,  4.15it/s, H_fake=0, H_real=0]\n"]},{"output_type":"stream","name":"stdout","text":["=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=* \n"," saving checkpoint\n","=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=* \n"," saving checkpoint\n","=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=* \n"," saving checkpoint\n","=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=* \n"," saving checkpoint\n"]}],"source":["\n","# train file\n","\n","import torch\n","# from dataset import HorseZebraDataset\n","import sys\n","# from utils import save_checkpoint, load_checkpoint\n","from torch.utils.data import DataLoader\n","import torch.nn as nn\n","import torch.optim as optim\n","# import config\n","from tqdm import tqdm\n","from torchvision.utils import save_image\n","# from discriminator_model import Discriminator\n","# from generator_model import Generator\n","\n","def train_fn(disc_H, disc_Z, gen_Z, gen_H, loader, opt_disc, opt_gen, l1, mse, d_scaler, g_scaler):\n","    H_reals = 0\n","    H_fakes = 0\n","    loop = tqdm(loader, leave=True)\n","\n","    for idx, (zebra, horse) in enumerate(loop):\n","        zebra = zebra.to(DEVICE)\n","        horse = horse.to(DEVICE)\n","        with torch.cuda.amp.autocast():\n","            fake_horse = gen_H(zebra)\n","            D_H_real = disc_H(horse)\n","            D_H_fake = disc_H(fake_horse.detach())\n","            D_H_real_loss = mse(D_H_real, torch.ones_like(D_H_real))\n","            D_H_fake_loss = mse(D_H_fake, torch.zeros_like(D_H_fake))\n","            D_H_loss = D_H_real_loss + D_H_fake_loss\n","\n","            fake_zebra = gen_H(horse)\n","            D_Z_real = disc_H(zebra)\n","            D_Z_fake = disc_H(fake_zebra.detach())\n","            D_Z_real_loss = mse(D_Z_real, torch.ones_like(D_Z_real))\n","            D_Z_fake_loss = mse(D_Z_fake, torch.zeros_like(D_Z_fake))\n","            D_Z_loss = D_Z_real_loss + D_Z_fake_loss\n","\n","            D_loss = (D_H_loss + D_Z_loss)\n","\n","        opt_disc.zero_grad()\n","        d_scaler.scale(D_loss).backward()\n","        d_scaler.step(opt_disc)\n","        d_scaler.update()\n","\n","        with torch.cuda.amp.autocast():\n","            D_H_fake = disc_H(fake_horse)\n","            D_Z_fake = disc_Z(fake_zebra)\n","            loss_G_H = mse(D_H_fake, torch.ones_like(D_H_fake))\n","            loss_G_Z = mse(D_Z_fake, torch.ones_like(D_Z_fake))\n","\n","            cycle_zebra = gen_Z(fake_horse)\n","            cycle_horse = gen_H(fake_zebra)\n","            cycle_zebra_loss = l1(zebra, cycle_zebra)\n","            cycle_horse_loss = l1(horse, cycle_horse)\n","\n","            identity_zebra = gen_Z(zebra)\n","            identity_horse = gen_H(horse)\n","            identity_zebra_loss = l1(zebra, identity_zebra)\n","            identity_horse_loss = l1(horse, identity_horse)\n","\n","            # add all togethor\n","            G_loss = (\n","                loss_G_Z\n","                + loss_G_H\n","                + cycle_zebra_loss * LAMBDA_CYCLE\n","                + cycle_horse_loss * LAMBDA_CYCLE\n","                + identity_horse_loss * LAMBDA_IDENTITY\n","                + identity_zebra_loss * LAMBDA_IDENTITY\n","            )\n","\n","        opt_gen.zero_grad()\n","        g_scaler.scale(G_loss).backward()\n","        g_scaler.step(opt_gen)\n","        g_scaler.update()\n","\n","        if idx % 200 == 0:\n","            save_image(fake_horse*0.5+0.5, f\"/content/saved_images/horse_{idx}.png\")\n","            save_image(fake_zebra*0.5+0.5, f\"/content/saved_images/zebra_{idx}.png\")\n","\n","        loop.set_postfix(H_real=H_reals/(idx+1), H_fake=H_fakes/(idx+1))\n","\n","\n","\n","def main():\n","    disc_H = Discriminator(in_channels=3).to(DEVICE)\n","    disc_Z = Discriminator(in_channels=3).to(DEVICE)\n","    gen_H = Generator(img_channels=3, num_residuals=9).to(DEVICE)\n","    gen_Z = Generator(img_channels=3, num_residuals=9).to(DEVICE)\n","\n","    opt_disc = optim.Adam(\n","        list(disc_H.parameters()) + list(disc_Z.parameters()),\n","        lr = LEARNING_RATE,\n","        betas = (0.5, 0.999)\n","    )\n","    opt_gen = optim.Adam(\n","        list(gen_H.parameters()) + list(gen_Z.parameters()),\n","        lr = LEARNING_RATE,\n","        betas = (0.5, 0.999)\n","    )\n","\n","    l1 = nn.L1Loss()\n","    mse = nn.MSELoss()\n","\n","    if LOAD_MODEL:\n","        load_checkpoint(\n","            CHECKPOINT_GEN_H, gen_H, opt_gen, LEARNING_RATE,\n","        )\n","        load_checkpoint(\n","            CHECKPOINT_GEN_Z, gen_Z, opt_gen, LEARNING_RATE,\n","        )\n","        load_checkpoint(\n","            CHECKPOINT_CRITIC_H, disc_H, opt_disc, LEARNING_RATE,\n","        )\n","        load_checkpoint(\n","            CHECKPOINT_CRITIC_Z, disc_Z, opt_disc, LEARNING_RATE,\n","        )\n","    dataset = HorseZebraDataset(TRAIN_DIR+'/trainA', TRAIN_DIR+'/trainB', transform=transforms)\n","\n","    loader = DataLoader(\n","        dataset,\n","        batch_size= BATCH_SIZE,\n","        shuffle=True,\n","        num_workers=NUM_WORKERS,\n","        pin_memory=True\n","    )\n","    g_scaler = torch.cuda.amp.GradScaler()\n","    d_scaler = torch.cuda.amp.GradScaler()\n","\n","    for epoch in range(NUM_EPOCHES):\n","        train_fn(disc_H, disc_Z, gen_Z, gen_H, loader, opt_disc, opt_gen, l1, mse, d_scaler, g_scaler)\n","\n","        if SAVE_MODEL:\n","            save_checkpoint(gen_H, opt_gen, filename=CHECKPOINT_GEN_H)\n","            save_checkpoint(gen_Z, opt_gen, filename=CHECKPOINT_GEN_Z)\n","            save_checkpoint(disc_H, opt_disc, filename=CHECKPOINT_CRITIC_H)\n","            save_checkpoint(disc_Z, opt_disc, filename=CHECKPOINT_CRITIC_Z)\n","\n","# if __name__ == '__main__':\n","os.makedirs('/content/saved_images', exist_ok = True)\n","os.makedirs('/content/cpt', exist_ok= True)\n","\n","main()"]},{"cell_type":"code","source":[],"metadata":{"id":"wHAQTR3dtcoQ"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"name":"cycle_gan.ipynb","provenance":[],"authorship_tag":"ABX9TyPXE3IOM5iK5ktcaYqXf5yu"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}