{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"BeV8NW5DbXi7"},"outputs":[],"source":["\"\"\"\n","model.py\n","Discriminator and Generator implementation from DCGAN paper\n","\"\"\"\n","\n","import torch\n","import torch.nn as nn\n","\n","\n","class Discriminator(nn.Module):\n","    def __init__(self, channels_img, features_d, num_classes, img_size):\n","        super(Discriminator, self).__init__()\n","        self.img_size = img_size\n","        self.disc = nn.Sequential(\n","            # input: N x channels_img x 64 x 64\n","            nn.Conv2d(channels_img+1, features_d, kernel_size=4, stride=2, padding=1),\n","            nn.LeakyReLU(0.2),\n","            # _block(in_channels, out_channels, kernel_size, stride, padding)\n","            self._block(features_d, features_d * 2, 4, 2, 1),\n","            self._block(features_d * 2, features_d * 4, 4, 2, 1),\n","            self._block(features_d * 4, features_d * 8, 4, 2, 1),\n","            # After all _block img output is 4x4 (Conv2d below makes into 1x1)\n","            nn.Conv2d(features_d * 8, 1, kernel_size=4, stride=2, padding=0),\n","        )\n","        self.embed = nn.Embedding(num_classes, img_size*img_size)\n","\n","    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n","        return nn.Sequential(\n","            nn.Conv2d(\n","                in_channels, out_channels, kernel_size, stride, padding, bias=False,\n","            ),\n","            nn.InstanceNorm2d(out_channels, affine=True),\n","            nn.LeakyReLU(0.2),\n","        )\n","\n","    def forward(self, x, labels):\n","        embedding = self.embed(labels).view(labels.shape[0], 1, self.img_size, self.img_size)\n","        x = torch.cat([x, embedding], dim=1)\n","        return self.disc(x)\n","\n","\n","class Generator(nn.Module):\n","    def __init__(self, channels_noise, channels_img, features_g, num_classes, img_size, embed_size):\n","        super(Generator, self).__init__()\n","        self.img_size = img_size\n","        self.net = nn.Sequential(\n","            # Input: N x channels_noise x 1 x 1\n","            self._block(channels_noise+embed_size, features_g * 16, 4, 1, 0),  # img: 4x4\n","            self._block(features_g * 16, features_g * 8, 4, 2, 1),  # img: 8x8\n","            self._block(features_g * 8, features_g * 4, 4, 2, 1),  # img: 16x16\n","            self._block(features_g * 4, features_g * 2, 4, 2, 1),  # img: 32x32\n","            nn.ConvTranspose2d(\n","                features_g * 2, channels_img, kernel_size=4, stride=2, padding=1\n","            ),\n","            # Output: N x channels_img x 64 x 64\n","            nn.Tanh(),\n","        )\n","        self.embed = nn.Embedding(num_classes, embed_size)\n","\n","    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n","        return nn.Sequential(\n","            nn.ConvTranspose2d(\n","                in_channels, out_channels, kernel_size, stride, padding, bias=False,\n","            ),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(),\n","        )\n","\n","    def forward(self, x, labels):\n","        embedding = self.embed(labels).unsqueeze(2).unsqueeze(3)\n","        x = torch.cat([x, embedding], dim=1)\n","        return self.net(x)\n","\n","\n","def initialize_weights(model):\n","    # Initializes weights according to the DCGAN paper\n","    for m in model.modules():\n","        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n","            nn.init.normal_(m.weight.data, 0.0, 0.02)\n","\n","\n","# def test():\n","#     N, in_channels, H, W = 8, 3, 64, 64\n","#     noise_dim = 100\n","#     x = torch.randn((N, in_channels, H, W))\n","#     disc = Discriminator(in_channels, 8, 10, 64)\n","#     assert disc(x, ).shape == (N, 1, 1, 1), \"Discriminator test failed\"\n","#     gen = Generator(noise_dim, in_channels, 8)\n","#     z = torch.randn((N, noise_dim, 1, 1))\n","#     assert gen(z).shape == (N, in_channels, H, W), \"Generator test failed\"\n","\n","\n","# test()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"jxG0fPnT1Rr8"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"V-_tsithbY2q"},"outputs":[],"source":["#utils.py\n","import torch\n","import torch.nn as nn\n","\n","\n","def gradient_penalty(critic, labels, real, fake, device=\"cpu\"):\n","    BATCH_SIZE, C, H, W = real.shape\n","    alpha = torch.rand((BATCH_SIZE, 1, 1, 1)).repeat(1, C, H, W).to(device)\n","    interpolated_images = real * alpha + fake * (1 - alpha)\n","\n","    # Calculate critic scores\n","    mixed_scores = critic(interpolated_images, labels)\n","\n","    # Take the gradient of the scores with respect to the images\n","    gradient = torch.autograd.grad(\n","        inputs=interpolated_images,\n","        outputs=mixed_scores,\n","        grad_outputs=torch.ones_like(mixed_scores),\n","        create_graph=True,\n","        retain_graph=True,\n","    )[0]\n","    gradient = gradient.view(gradient.shape[0], -1)\n","    gradient_norm = gradient.norm(2, dim=1)\n","    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n","    return gradient_penalty\n","\n","\n","def save_checkpoint(state, filename=\"celeba_wgan_gp.pth.tar\"):\n","    print(\"=\u003e Saving checkpoint\")\n","    torch.save(state, filename)\n","\n","\n","def load_checkpoint(checkpoint, gen, disc):\n","    print(\"=\u003e Loading checkpoint\")\n","    gen.load_state_dict(checkpoint['gen'])\n","    disc.load_state_dict(checkpoint['disc'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"output_embedded_package_id":"1XAxpoC0EnUnA18GCFR9Czbk9pwqwM6a1"},"id":"yiU_oCTKbgHi","outputId":"5dcfca61-8f83-4888-a61e-38d150f5347f"},"outputs":[],"source":["#train.py\n","\n","\"\"\"\n","Training of WGAN-GP\n","\"\"\"\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision\n","import torchvision.datasets as datasets\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader\n","from torch.utils.tensorboard import SummaryWriter\n","# from utils import gradient_penalty, save_checkpoint, load_checkpoint\n","# from model import Discriminator, Generator, initialize_weights\n","\n","# Hyperparameters etc.\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","LEARNING_RATE = 1e-4\n","BATCH_SIZE = 64\n","IMAGE_SIZE = 64\n","NUM_CLASSES = 10\n","GEN_EMBEDDING = 100\n","CHANNELS_IMG = 1\n","Z_DIM = 100\n","NUM_EPOCHS = 100\n","FEATURES_CRITIC = 16\n","FEATURES_GEN = 16\n","CRITIC_ITERATIONS = 5\n","LAMBDA_GP = 10\n","\n","transforms = transforms.Compose(\n","    [\n","        transforms.Resize(IMAGE_SIZE),\n","        transforms.ToTensor(),\n","        transforms.Normalize(\n","            [0.5 for _ in range(CHANNELS_IMG)], [0.5 for _ in range(CHANNELS_IMG)]),\n","    ]\n",")\n","\n","dataset = datasets.MNIST(root=\"dataset/\", transform=transforms, download=True)\n","# comment mnist above and uncomment below for training on CelebA\n","#dataset = datasets.ImageFolder(root=\"celeb_dataset\", transform=transforms)\n","loader = DataLoader(\n","    dataset,\n","    batch_size=BATCH_SIZE,\n","    shuffle=True,\n",")\n","\n","# initialize gen and disc, note: discriminator should be called critic,\n","# according to WGAN paper (since it no longer outputs between [0, 1])\n","gen = Generator(Z_DIM, CHANNELS_IMG, FEATURES_GEN, NUM_CLASSES, IMAGE_SIZE, GEN_EMBEDDING).to(device)\n","critic = Discriminator(CHANNELS_IMG, FEATURES_CRITIC, NUM_CLASSES, IMAGE_SIZE).to(device)\n","initialize_weights(gen)\n","initialize_weights(critic)\n","\n","# initializate optimizer\n","opt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.9))\n","opt_critic = optim.Adam(critic.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.9))\n","\n","# for tensorboard plotting\n","fixed_noise = torch.randn(32, Z_DIM, 1, 1).to(device)\n","writer_real = SummaryWriter(f\"logs/GAN_MNIST/real\")\n","writer_fake = SummaryWriter(f\"logs/GAN_MNIST/fake\")\n","step = 0\n","\n","gen.train()\n","critic.train()\n","\n","for epoch in range(NUM_EPOCHS):\n","    # Target labels not needed! \u003c3 unsupervised\n","    for batch_idx, (real, labels) in enumerate(loader):\n","        real = real.to(device)\n","        cur_batch_size = real.shape[0]\n","        labels = labels.to(device)\n","        # Train Critic: max E[critic(real)] - E[critic(fake)]\n","        # equivalent to minimizing the negative of that\n","        for _ in range(CRITIC_ITERATIONS):\n","            noise = torch.randn(cur_batch_size, Z_DIM, 1, 1).to(device)\n","            fake = gen(noise, labels)\n","            critic_real = critic(real, labels).reshape(-1)\n","            critic_fake = critic(fake, labels).reshape(-1)\n","            gp = gradient_penalty(critic, labels, real, fake, device=device)\n","            loss_critic = (\n","                -(torch.mean(critic_real) - torch.mean(critic_fake)) + LAMBDA_GP * gp\n","            )\n","            critic.zero_grad()\n","            loss_critic.backward(retain_graph=True)\n","            opt_critic.step()\n","\n","        # Train Generator: max E[critic(gen_fake)] \u003c-\u003e min -E[critic(gen_fake)]\n","        gen_fake = critic(fake, labels).reshape(-1)\n","        loss_gen = -torch.mean(gen_fake)\n","        gen.zero_grad()\n","        loss_gen.backward()\n","        opt_gen.step()\n","\n","        # Print losses occasionally and print to tensorboard\n","        if batch_idx % 100 == 0 and batch_idx \u003e 0:\n","            print(\n","                f\"Epoch [{epoch}/{NUM_EPOCHS}] Batch {batch_idx}/{len(loader)} \\\n","                  Loss D: {loss_critic:.4f}, loss G: {loss_gen:.4f}\"\n","            )\n","\n","            with torch.no_grad():\n","                fake = gen(fixed_noise, labels)\n","                # take out (up to) 32 examples\n","                img_grid_real = torchvision.utils.make_grid(real[:32], normalize=True)\n","                img_grid_fake = torchvision.utils.make_grid(fake[:32], normalize=True)\n","\n","                fake_16 = fake.view(-1, 64, 64)[:16]\n","                import matplotlib.pyplot as plt\n","                fig = plt.figure(figsize=(4,4))\n","                for i in range(16):\n","                    plt.subplot(4,4,i+1)\n","                    plt.imshow(fake_16[i].to('cpu'), cmap='gray')\n","                    plt.axis('off')\n","                plt.show()\n","\n","                writer_real.add_image(\"Real\", img_grid_real, global_step=step)\n","                writer_fake.add_image(\"Fake\", img_grid_fake, global_step=step)\n","\n","            step += 1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"oj6JJuWM0Cpz"},"outputs":[],"source":["import numpy as np\n","a1, a2 = np.arange(12).reshape(-1, 3), np.arange(12, 24).reshape(-1, 3)\n","a1, a2"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"9REZoJgFl9FF"},"outputs":[],"source":["c = np.concatenate((a1, a2), axis=0)\n","c, c.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"W67-ssO4mnQZ"},"outputs":[],"source":[""]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyPlm4mwdzYR0RROxJOvBv2x","name":"conditional_gan.ipynb","version":""},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}